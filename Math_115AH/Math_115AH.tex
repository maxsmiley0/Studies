\documentclass[12pt]{article}
\usepackage{geometry, amsfonts, parskip, amssymb}
\pagestyle{empty}

\begin{document}

\title{Math 115AH}
\author{Max Smiley}
\date{Winter 2021}
\maketitle
This is an honors, junior level linear algebra class I took at UCLA. There were three broad categories of the course: Vector Spaces, Linear Transformations, and Inner Product Spaces. In each section, I'll go over definitions, theorems, and some important proofs and corollaries in chronological order of the course. I'll also attempt to give extra commentary and applications to things I find interesting. This is an informal set of notes, mainly intended to be referenced by myself down the road.
\clearpage

\textbf{\Large Vector Spaces}
\\\\
\textbf{Def: }a \emph{field} is a set $F$ with two maps $+: F \times F \rightarrow F$ and $\cdot: F \times F \rightarrow F$ satisfying the following properties\\

\begin{itemize}
\item $\forall\ a, b, c \in F,\ (a + b) + c = a + (b + c)$
\item $\forall\ a, b \in F,\ a + b = b + a$
\item $\exists ^^21 \ 0 \in F,\ such\ that\ a + 0 = 0 + a\quad \forall\ a \in F$
\item $\forall\ a \in F,\ \exists\ b \in F,\ such\ that\ a + b = b + a = 0.\ We\ write\ a = -b$
\item $\forall\ a,b,c \in F,\ (a \cdot b) \cdot c = a \cdot (b \cdot c)$
\item $\forall\ a,b \in F,\ a \cdot b = b \cdot a$
\item $\exists ^^21 \ 1 \in F,\ such\ that\ 1 \cdot a = a \cdot 1 = a \quad \forall\ a \in F$
\item $\forall\ a \neq 0\in F,\ \exists ^^21\ b\in F\ such\ that\ a \cdot b = b \cdot a = 1.\ We\ write\ a = b^{-1}$
\item $\forall\ a,b,c \in F,\ a \cdot (b + c) = a \cdot b + a \cdot c$
\item $\forall\ a,b,c \in F,\ a \cdot (b + c) \in F$
\end{itemize}

This defines two operators on a set, addition and multiplication. Essentially, these rules break down as follows - associativity, commutativity, existence (and uniqueness) of inverses, distributivity and closure of addition and multiplication.\\

A few examples of fields are the set of all rational numbers $\mathbb{Q}$, the set of all real numbers $\mathbb{R}$, and the set of all complex numbers $\mathbb{C}$. Note that the set of all integers $\mathbb{Z}$ does not form a field, as their multiplicative inverses are \emph{not} in the field.\\

Note that we also made no mention of what these operations actually had to represent, only that they must follow those four rules. This allows for a greater degree of generality - the multiplication and addition operators don't necessarily have to be what we're used to them being. Defining the notion of a field is also useful, as if we can prove a statement is true about a field, we can apply it to \emph{any} set that fulfills this description. Namely, when we define vector spaces, we define them over a field, which allows us to prove theorems for the reals, complexes, or rationals to name a few. \\

Though all of the fields I named in the previous paragraph were infinite sets, fields can be finite. Consider the following set, and tables of operations.\\

$$F = \{0, 1\}$$

\begin{center}
\begin{tabular}{ c | c c }
$+$ & 0 & 1 \\
\hline
0 & 0 & 1 \\
1 & 1 & 0
\end{tabular}
\qquad \qquad \qquad
\begin{tabular}{ c | c c }
$\cdot$ & 0 & 1 \\
\hline
0 & 0 & 0 \\
1 & 0 & 1
\end{tabular}
\end{center}
This satisfies all of the field axioms.\\

\textbf{Def: }a \emph{vector space} over a field F is a set V with two operations $+:V \times V \rightarrow V$ and $\cdot : F\times V \rightarrow V$ (referred to as vector addition and scalar multiplication) satisfying the following properties

\begin{itemize}
\item $\forall\ v_1, v_2, v_3 \in V,\ (v_1 + v_2) + v_3 = v_1 + (v_2 + v_3)$
\item $\forall\ v_1, v_2 \in V,\ v_1 + v_2 = v_2 + v_1$
\item $\exists ^^21 \ 0 \in V,\ such\ that\ v + 0 = 0 + v\quad \forall\ v \in V$
\item $\forall\ v \in V,\ \exists\ w \in V,\ such\ that\ v + w = w + v = 0.\ We\ write\ v = -w$
\item $\forall\ v \in V,\ v \cdot 1 = 1 \cdot v = v$
\item $\forall\ v \in V,\ and\ \alpha,\beta \in F,\ \alpha \cdot (\beta \cdot v) = (\alpha \cdot \beta) \cdot v$
\item $\forall\ v \in V,\ and\ \alpha,\beta \in F,\ (\alpha + \beta) \cdot v = \alpha \cdot v + \beta \cdot v$
\item $\forall\ v_1, v_2 \in V,\ and\ \alpha \in F,\ \alpha \cdot (v_1 + v_2) = \alpha \cdot v_1 + \alpha \cdot v_2$
\item $\forall\ v_1, v_2 \in V,\ and\ \alpha \in F,\ \alpha \cdot v_1 + v_2 \in F$
\end{itemize}
These axioms looks suspiciously similar to the field axioms. In fact, \emph{any field is a vector space over itself}. By this construction, the vector space axioms are inherited by the field axioms. For example, the field $\mathbb{R}$ is a vector space over $\mathbb{R}$. To give an example of a vector space that's a bit more interesting, the set of 3 element vectors over the field $\mathbb{R}$ forms a vector space, denoted as $\mathbb{R}$$^3$. In general, the set of all n element vectors over the a field F form the vector space denoted as $F^n$.

\textbf{Def: }a \emph{subspace} of a vector space V is a subset $W \subseteq V$ such that W is also a vector space.\\\\
\textbf{Subspace Theorem: } \emph{Let V be a vector space over the field F, and let $W \subseteq V$ be nonempty. Then the following are equivalent.}
\begin{itemize}
\item W is a subspace of V
\item W is closed under addition and scalar multiplication
\item $\forall\ w_1, w_2 \in W\ and\ \alpha \in F,\ \alpha \cdot w_1 + w_2 \in W$
\end{itemize}
\emph{Proof: }1) $\Rightarrow$ 2) by vector space axioms. 2) $\Rightarrow$ 3)  by definition. Now assume 3). Take $\alpha = 1$ to get $w_1 + w_2 \in W\ \forall\ w_1, w_2 \in W$. Take $\alpha = -1$ to get $w_2 - w_1 \in W\ \forall\ w_1, w_2 \in W$. Finally, take $w_2 = 0_V \in W$ to get $\alpha \cdot w_1 \in W\ \forall\ \alpha \in F$ and $w_1 \in W$. Thus, W is closed under addition and scalar multiplication, so it is a subspace, and so 3) $\Rightarrow$ 1).\\

\textbf{Def: }let V be a vector space over a field F. A \emph{linear combination} of elements in V is a sum of multiples of elements of V. For example, $\alpha v_1 + \beta v_2 + \gamma v_3$ is a linear combination for $v_1,v_2,v_3 \in V$ and $\alpha, \beta, \gamma \in F$. A vector $v \in V$ is said to be a \emph{linear combination} if $\exists$ scalars $\alpha_i \in F$ and $\exists\ v_i \in V$ such that $v = \sum_{i}^{}\alpha_i v_i$.\\

\textbf{Def: }let V be a vector space over a field F, and $\beta =  \{ v_1 \dots v_n  \}$ a set of n vectors in V. The \emph{span} of $\beta$ is the set of all linear combinations of elements of $\beta$ with scalars from F.
$$span(v_1,\dots v_n) = \sum_{i = 1}^{n}\alpha_i v_i \quad \forall\ \alpha_i \in F,\ \forall\ v_i \in V$$
Note that if $v_1,\dots v_n \in V$, then $span(v_1,\dots v_n)$ is a subspace of V. It is a subset, and is closed under addition and scalar multiplication.\\

\textbf{Def: }let $W_1, W_2 \subseteq V$ be subspaces. We define the direct sum $$W_1 + W_2 = \{ w_1 + w_2\ :\ w_1 \in W_1,\ w_2 \in W_2 \} = span(W_1 \cup W_2)$$\\
More generally, if $W_i \subseteq V$ are subspaces indexed by I, we define$$\sum_{I}W_i = span\Bigg(\bigcup_{I}W_i \Bigg)$$\\
This is a subspace, due to the way in which we defined span. Note that if we had defined $+$ by taking the union of the two sets, this would not form a subspace, as the unions of two subspaces are not necessarily closed under addition between the two elements.

\textbf{Def: }a set of vectors in a vector space V is \emph{linearly independent} if $$\sum_{i = 1}^{n}\alpha_i v_i = 0 \Rightarrow \alpha_i = 0\ \forall \alpha_i \in F,\ \forall v_i \in V\ for\ not\ all\ v_i = 0$$
Likewise, a set of vectors is \emph{linearly dependent} if $$\exists\ \alpha_i \neq 0\ such\ that\ \sum_{i = 1}^{n}\alpha_i v_i = 0\ is\ true\ for\ not\ all\ v_i = 0$$\\
Linear (in)dependence is a big deal in linear algebra. Without diving into the mathematics of it (yet), linearly dependent vectors mean that some vector in the set can be expressed by other vectors in the set. In a sense, there is some repeated information. In contrast, each newly added linearly independent vector to a set adds new information.\\

\textbf{Toss In Theorem: }\emph{Let V be a vector space over F, and $\varnothing \neq S \subseteq V$ be a linearly independent subset of V. Suppose $v \in V$ is not in the span of S. Then $S \cup \{ v \}$ is linearly independent.}\\\\
\emph{Proof: }Suppose $S\ \cup\ \{ v \}$ is linearly dependent. Then $\exists\ v_1,\dots v_n \in S\ and\ \alpha_i,\dots \alpha_n \in F$ not all zero such that $$\alpha_1 v_1 + \dots + \alpha_n v_n + \alpha v= 0$$\\
Suppose $\alpha = 0$. Then our equation reads $\alpha_1 v_1 + \dots\ + \alpha_n v_n = 0$ for not all $\alpha_i = 0$. However, this contradicts the linear independence of S.\\

Now suppose $\alpha \neq 0$. Then $\alpha^{-1}$ exists, so we can multiply both sides and rearrange to arrive at $$v =  -\alpha^{-1} \alpha_1 v_1 - \dots\ - \alpha^{-1} \alpha_n v_n$$\\
The above is simply a linear combination of elements of S, so it is in the span of S. However, we initially assumed $v \notin span(S)$, and so we have arrived at a contradiction. Since all scenarios lead to a contradiction, we can conclude what we had assumed is false, proving that $S \cup \{ v \}$ is linearly independent.\\

\textbf{Def: }a \emph{basis} for a vector space V is a set of linearly independent vectors in V that spans V.\\\\
\textbf{Def: }for a given basis $\beta = \{ v_1, \dots v_n \}$ of a vector space V, the \emph{coordinates} refer to the scalars associated with the basis vectors. For example, an arbitrary $v \in V$ has representation $$v = \alpha_1 v_1 + \dots + \alpha_n v_n \quad \forall\ \alpha_i \in F$$\\as $\beta$ spans V. $\alpha_i$ is referred to as the "i'th" coordinate of $\beta$, associated with the basis vector $v_i$. \\

\textbf{Coordinate Theorem: }\emph{for any given basis $\beta = \{ v_1,\dots v_n\}$ the coordinates associated with a given element are unique}\\

\emph{Proof: }Suppose we have two equivalent vectors with representations $$v = \alpha_1 v_1 + \dots + \alpha_n v_n \ and\ v = \beta_1 v_1 + \dots + \beta_n v_n \Rightarrow (\alpha_1 - \beta_1) v_1 + \dots + (\alpha_n - \beta_n) v_n = 0$$
However, $\beta$ is linearly independent by definition of a basis, implying $\alpha_i - \beta_i = 0\ \forall i$
$\Rightarrow \alpha_i = \beta_i\ \forall i$ and so the coordinates are unique.\\

\textbf{Toss Out Theorem: }\emph{Suppose $v \in span(v_1,\dots v_n)$. Then $span(v, v_1,\dots v_n) = span(v_1 \dots v_n)$}. In plain text, we can "toss out" a linearly dependent vector from a set to maintain the same span.\\

\emph{Proof: }Let $W = span(v, v_1,\dots v_n)$ and $W' = span(v_1,\dots v_n)$. Clearly, $W' \subseteq W$. Now suppose $w \in W$. Then $w = \alpha v + \alpha_1 v_1 + \dots \alpha_n v_n$ for some $\alpha, \alpha_1 \dots \alpha_n \in F$. But since $v \in W',\ v = \beta_1 v_1 + \dots + \beta_n v_n$ for some $\beta_1,\dots \beta_n \in F$
$$\Rightarrow w = \alpha (\beta_1 v_1 + \dots + \beta_n v_n) + \alpha_1 v_1 + \dots + \alpha_n v_n$$
$$\Rightarrow w = (\alpha \beta_1 + \alpha_1) v_1 + \dots + (\alpha \beta_n + \alpha_n) v_n$$\\
Since $w$ can be represented purely as a linear combination of elements in $W'$, $$\Rightarrow w \in W' \Rightarrow W \subseteq W' \Rightarrow W = W'$$

\textbf{Replacement Theorem: }\emph{Let V be a vector space over a field F with a given basis $\beta = \{ v_1,\dots v_n \}$. Suppose that $v \in V$ is a linear combination of the basis vectors, with some coordinate $\alpha_i \neq 0$. Then $\beta ' = \{ v_1,\dots v_{i - 1}, v, v_{i + 1},\dots v_n \}$ is also a basis for V.} In plain text, if we find some vector with a nonzero component along the \emph{i'th} basis vector, we can form a new basis by replacing that basis vector with our new one.\\

\emph{Proof: }Without loss of generality, assume $i = 1$. The $\alpha_i^{-1}$ exists, and so for an arbitrary $v \in V,\ v = \alpha_1 v_1 + \dots + \alpha_n v_n$, we can rewrite this to conclude $$v_1 = \alpha_1^{-1} v - \alpha_1^{-1} \alpha_2 v_2 - \dots - \alpha_1^{-1} \alpha_n v_n \Rightarrow v_1 \in span(v, v_2,\dots v_n)$$
Since v is simply a linear combination of $v_1, \dots v_n$, it is evident that $span(v_1,\dots v_n) = span(v, v_1, \dots v_n).$ However, since $v_1 \in span(v,v_2,\dots v_n)$, by the Toss Out Theorem, $span(v, v_1, \dots v_n) = span(v,v_2,\dots, v_n)$. Hence, $span(\beta) = span(\beta')$, and so $\beta'$ spans V. Thus, it suffices to show $\beta'$ is a basis if it is linearly independent. By way of contradiction, assume $\beta'$ is linearly dependent.\\
$\Rightarrow \exists\ \beta, \beta_2, \dots \beta_n $ not all zero such that $\beta v + \beta_2 v_2 + \dots + \beta_n v_n = 0$.\\

Case 1: $\beta = 0$\\
Then this implies $\beta_2 v_2 + \dots + \beta_n v_n = 0$ for $\beta_2 \dots \beta_n$ not all equal to zero. However, this contradicts their linear independence, as $\beta = \{ v_1 \dots v_n\}$ is linearly independent.\\

Case 2: $\beta \neq 0$\\
Then $\beta^{-1}$ exists, so we can multiply both sides by this, solve for $v$, and set it equal to our original equation for v.
$$v = 0 \cdot v_1 -\beta^{-1} \beta_2 v_2 - \dots - \beta^{-1} \beta_n v_n = \alpha_1 v_1 + \dots + \alpha_n v_n$$\\
By the coordinate theorem, $\alpha_1 = 0$. However, this is exactly the $\alpha_i$ we had assumed to be nonzero, so we have arrived at a contradiction. In either case, the assumption that $\beta'$ is linearly dependent leads to a contradiction, and so $\beta'$ is linearly independent.
\clearpage
\textbf{Main Theorem: }\emph{Suppose V is a vector space over a field F with $V = span(v_1,\dots v_n)$. Then any linearly independent subset in V has at most n elements.}\\

\emph{Proof: }by the Toss Out Theorem, we can toss out linearly dependent vectors from this set without affecting the span, until we have arrived at a linearly independent set. Since this set spans V and is linearly independent, it is a basis for V. Assume $\beta = \{ v_1, \dots, v_n \}$ is a basis for V. Now suppose $\{ w_1, \dots w_m \} \subseteq V$ is a linearly independent set. We must show $m \leq n$. We claim it suffices to show that for each $k \leq n$, $\{ w_1, \dots w_k, v_{k + 1}, \dots v_n \}$ is a basis. If $k < n$, then $m < n$, and so the theorem is proved. If $k = n$, then $\{ w_1, \dots w_n \}$ is a basis, as any $w_{k + c}$ for $c > 0$ implies $w_{k + c}$ can be written as a linear combination of $\{ w_1, \dots, w_n \}$, i.e. the $w$'s are not linearly independent, contradicting the claim that the $w$'s form a basis.\\

Base Case: $k = 1$.\\
Then $w_1 = \alpha_1 v_1 + \dots + \alpha_n v_n$ for some $\alpha_i \in F$. Since $w_1$ is a basis element, some $\alpha_i$ is nonzero, so without loss of generality, assume $\alpha_1 \neq 0$. Rearranging, we can express $v_1$ as a linear combination of $\{ w_1, v_2, \dots, v_n \}$, and so by the Replacement Theorem, $\{ w_1, v_2, \dots, v_n \}$ is a basis for V.\\

Inductive Step: $does\ P(n-1) \Rightarrow P(n)?$\\
Assume $P(n - 1)$, i.e. for some $k < n$, $\{ w_1, \dots w_k, v_{k + 1}, \dots v_n \}$ is a basis. We want to show that $\{ w_1, \dots w_{k + 1}, v_{k + 2}, \dots v_n \}$ is a basis. $w_{k + 1}$ has representation $w_{k + 1} = \alpha_1 w_1 + \dots + \alpha_k w_k + \beta_{k + 1} v_{k + 1} + \dots \beta_{n} v_n$. Note $\exists\ \beta_i \neq 0$, as if all $\beta_i$ were $0$, then $w_{k + 1} \in span(w_1, \dots w_m)$, contradicting the linear independence of the $w$'s. Without loss of generality, assume $\beta_{k + 1} \neq 0$. Hence, $\{ w_1, \dots w_{k + 1}, v_{k + 2}, \dots v_n \}$ is a basis by the Replacement Theorem.\\

\textbf{Def: }the \emph{dimension} of a vector space V is the cardinality of a basis for V. This may be finite or infinite dimensional.\\

Many people, upon first coming to grips with the notion of an infinite dimensional vector space, are rightly confused. \emph{Aren't most vector spaces infinite sets?} This is true, but the dimension of a vector space represents something different than the mere size of the vector space itself. It represents \emph{how many basis vectors we need} in order to describe an arbitrary object in V through linear combination.
\clearpage
For example, consider $\mathbb{R}^2$, the set of all $(x, y)$ such that $x \in \mathbb{R}$ and $y \in \mathbb{R}$, or equivalently, $\mathbb{R} \times \mathbb{R}$. This is a finite dimensional vector space, as there exists a basis with only two elements, $(1, 0)$ and $(0, 1)$ to name an example.\\

In contrast, consider the set of all polynomials with coefficients over a field, denoted as $F[t]$, with an arbitrary element having representation $\alpha_0 + \alpha_1 t + \alpha_2 t^2 + \dots + \alpha_n t^n$ for $\alpha_0, \dots \alpha_n \in F$. This is an infinite dimensional vector space, since the monomials generated by $t^n$ form basis for $F[t]$, but since $n \in \mathbb{N}$, $n$ is not bounded by any finite number (the span of $t^n$ is evident, and their linear independence is a result from differential equations). It is important to make the distinction between infinite and finite dimensional vector spaces, as some theorems only hold in the finite dimensional case.\\

\textbf{Corollary: }\emph{Let V be a finite-dimensional vector space over a field F of dimension $n  > 0$, and $\varnothing \neq S \subseteq V$ a subset. Then}

\begin{itemize}
\item If $|S| > n$, then S is linearly dependent
\item If $|S| < n$, then S does not span V
\end{itemize}
The theorem states that a maximally linearly independent set in V is a basis. By the Toss Out Theorem, a minimal spanning set of V is a basis.\\

\textbf{Corollary: }\emph{Let V be a finite-dimensional vector space over a field F with bases $\beta_1$ and $\beta_2$. Then $|\beta_1| = |\beta_2|$.}\\

By definition, a basis $\beta$ must span V and be linearly independent. Then, by the previous corollary, $|\beta| \geq dim(V)$, and $|\beta| \leq dim(V)$, or $|\beta| = dim(V)$. Thus, for a given V, all bases have the same cardinality, and so $|\beta_1| = |\beta_2|$\\

This is what we expected, after all it would be disastrous if the dimension of a vector space depended on what basis we chose. Since we've also concluded that a basis is simply a minimal spanning set, we can conclude that every vector space has a basis. Evidently, there is an intimate relationship between bases and vector spaces. In fact, when we talk about linear transformations, much of our work will be describing what the linear transformations does to the basis elements, to give an example of how useful describing things in terms of their bases are.
\clearpage
\textbf{Extension Theorem: }\emph{Let V be a finite-dimensional vector space over a field F, and $W \subseteq V$ a subspace of V. Then any linearly independent subset of W is finite, and part of a basis for W.} It also follows that any linearly independent subset of V may be extended to a basis for V.\\

\emph{Proof: }By the main theorem, any linearly independent subset $\beta \subseteq W$ has at most $dim(W)$ linearly independent vectors. In particular, $dim(span(\beta)) \leq dim(W)$. If $dim(span(\beta)) = dim(W)$, we are done, as any linearly independent subset of a vector space with cardinality equal to the dimension of the given vector space forms a basis for that vector space. If $dim(span(\beta)) < dim(W)$, by the Toss In Theorem, we can add in some vector $v \in V$ where $v \notin span(\beta)$ such that $\beta \cup \{ v \}$ is linearly independent, and our new set $\beta'$ has dimension $|\beta| + 1$. Since V is finite dimensional, this process will eventually terminate when $dim(\beta') = dim(W)$, in which case $\beta'$ will be a basis for $W$ by the Main Theorem.\\

\textbf{Counting Theorem: }\emph{Let V be a vector space over F, with $W_1, W_2 \subseteq V$ finite-dimensional subsets of V. Then}
\begin{itemize}
\item $W_1 \cap W_2$ is a finite-dimensional vector space
\item $W_1 + W_2$ is a finite-dimensional vector space
\item $dim(W_1) + dim(W_2) = dim(W_1 + W_2) + dim(W_1 \cap W_2)$
\end{itemize}
\emph{Proof: }$W_1 \cap W_2 \subseteq W$, so it is finite-dimensional since W is finite-dimensional. Moreover, it is a vector space, as $\forall\ x, y \in W_1 \cap W_2$ and $\forall\ c \in F$
$$\Rightarrow x \in W_1, W_2\ and\ y \in W_1, W_2\Rightarrow cx + y \in W_1, W_2\ since\ W_1\ and\ W_2\ are\ subspaces$$  
If $\beta_1, \beta_2$ are bases for $W_1, W_2$ respectively, then by definition, $W_1 + W_2 = span(\beta_1 \cup \beta_2)$. Since $\beta_1, \beta_2$ are finite, then $W_1 + W_2$ is finite dimensional. Moreover, $\beta_1 \cup \beta_2$ can be reduces to a basis by the Toss Out Theorem, and so $W_1 + W_2$ is also a vector space.\\

Now suppose $\beta$ is a basis for $W_1 \cup W_2$. By the Extension Theorem, we can extend this to a basis for $W_1$ and $W_2$, denoted as $\beta_1,\beta_2$ respectively. We want to claim that $dim(W_1 + W_2) = dim(span(\beta_1 \cup \beta_2))$. To do so, we simply need to show $\beta_1 \cup \beta_2$ is linearly independent. By way of contradiction, assume $\beta_1 \cup \beta_2$ is linearly dependent. Note that given a general representation of a vector in V for our basis elements, it equaling zero implies not all coefficients associated with $\beta$ and $\beta_2$ are zero, as this would violate the linear independence of $\beta_1$. Similarly, not all coefficients regarding $\beta$ and $\beta_2$ are equal to zero. Then, if our basis vectors are $$\beta = \{ v_1, \dots v_n \}, \beta_1 = \{ y_1, \dots y_m \}, \beta_2 = \{ z_1, \dots z_r \}\ with\ scalars\ \alpha_1, \dots \alpha_n, \beta_1, \dots \beta_m, \gamma_1, \dots \gamma_r \in F$$
$$\Rightarrow v = \alpha_1 v_1 + \dots + \alpha_n v_n + \beta_1 y_1 + \dots + \beta_m y_m + \gamma_1 z_1 + \dots + \gamma_r z_r = 0$$
Without loss of generality, assume $\gamma_1 \neq 0$. Note our representations of z are
$$0 \neq z = \gamma_1 z_1 + \dots + \gamma_r z_r \in span(\beta_2) = W_2$$
$$0 \neq z = -\alpha_1 v_1 - \dots - \alpha_n v_n - \beta_1 y_1 - \dots - \beta_m y_m \in span(\beta_2) = W_2$$
$$\Rightarrow \gamma_1 z_1 + \dots + \gamma_r z_r = -\alpha_1 v_1 - \dots - \alpha_n v_n - \beta_1 y_1 - \dots - \beta_m y_m$$
$$\Rightarrow \exists\ \delta_1, \dots \delta_n\ such\ that\ 0 \neq z = \delta_1 v_1 + \dots + \delta_n v_n$$
$$\Rightarrow \delta_1 v_1 + \dots + \delta_n v_n + 0 \cdot z_1 + \dots + 0 \cdot z_r = \gamma_1 z_1 + \dots \gamma_r z_r$$
But, by the coordinate theorem we get $\gamma_1 = 0$, when we had assumed $\gamma_1 \neq 0$! Thus, we have arrived at a contradiction, and so $\beta_1 \cup \beta_2$ is linearly independent. Thus, $dim(W_1 + W_2) = dim(span(\beta_1 \cup \beta_2)$
$$ \Rightarrow dim (W_1 + W_2) = |\beta_1| + |\beta_2| - |\beta_1 \cap \beta_2|$$
$$ \Rightarrow dim(W_1 + W_2) = dim(W_1) + dim(W_2) - dim(W_1 \cap W_2)$$

At this point, we have fleshed out the concepts we need to describe vector spaces, and proved important theorems that we'll build upon in the next section. We defined fields, then built upon them with vector spaces. We related the notion of linear independence, span, basis, and dimension to each other, and rigorously proved ideas we may have taken for granted in our first pass at linear algebra.

\end{document}





































