\documentclass[12pt]{article}
\usepackage{geometry, amsfonts, parskip, amssymb}
\pagestyle{empty}

\begin{document}

\title{Math 115AH}
\author{Max Smiley}
\date{Winter 2021}
\maketitle
This is an honors, junior level linear algebra class I took at UCLA. There were three broad categories of the course: Vector Spaces, Linear Transformations, and Inner Product Spaces. In each section, I'll go over definitions, theorems, and some important proofs and corollaries in chronological order of the course. I'll also attempt to give extra commentary and applications to things I find interesting. This is an informal set of notes, mainly intended to be referenced by myself down the road.
\clearpage

\textbf{\Large Vector Spaces}
\\\\
\textbf{Def: }a \emph{field} is a set $F$ with two maps $+: F \times F \rightarrow F$ and $\cdot: F \times F \rightarrow F$ satisfying the following properties\\

\begin{itemize}
\item $\forall\ a, b, c \in F,\ (a + b) + c = a + (b + c)$
\item $\forall\ a, b \in F,\ a + b = b + a$
\item $\exists ^^21 \ 0 \in F,\ such\ that\ a + 0 = 0 + a\quad \forall\ a \in F$
\item $\forall\ a \in F,\ \exists\ b \in F,\ such\ that\ a + b = b + a = 0.\ We\ write\ a = -b$
\item $\forall\ a,b,c \in F,\ (a \cdot b) \cdot c = a \cdot (b \cdot c)$
\item $\forall\ a,b \in F,\ a \cdot b = b \cdot a$
\item $\exists ^^21 \ 1 \in F,\ such\ that\ 1 \cdot a = a \cdot 1 = a \quad \forall\ a \in F$
\item $\forall\ a \neq 0\in F,\ \exists ^^21\ b\in F\ such\ that\ a \cdot b = b \cdot a = 1.\ We\ write\ a = b^{-1}$
\item $\forall\ a,b,c \in F,\ a \cdot (b + c) = a \cdot b + a \cdot c$
\item $\forall\ a,b,c \in F,\ a \cdot (b + c) \in F$
\end{itemize}

This defines two operators on a set, addition and multiplication. Essentially, these rules break down as follows - associativity, commutativity, existence (and uniqueness) of inverses, distributivity and closure of addition and multiplication.\\

A few examples of fields are the set of all rational numbers $\mathbb{Q}$, the set of all real numbers $\mathbb{R}$, and the set of all complex numbers $\mathbb{C}$. Note that the set of all integers $\mathbb{Z}$ does not form a field, as their multiplicative inverses are \emph{not} in the field.\\

Note that we also made no mention of what these operations actually had to represent, only that they must follow those four rules. This allows for a greater degree of generality - the multiplication and addition operators don't necessarily have to be what we're used to them being. Defining the notion of a field is also useful, as if we can prove a statement is true about a field, we can apply it to \emph{any} set that fulfills this description. Namely, when we define vector spaces, we define them over a field, which allows us to prove theorems for the reals, complexes, or rationals to name a few. \\

Though all of the fields I named in the previous paragraph were infinite sets, fields can be finite. Consider the following set, and tables of operations.\\

$$F = \{0, 1\}$$

\begin{center}
\begin{tabular}{ c | c c }
$+$ & 0 & 1 \\
\hline
0 & 0 & 1 \\
1 & 1 & 0
\end{tabular}
\qquad \qquad \qquad
\begin{tabular}{ c | c c }
$\cdot$ & 0 & 1 \\
\hline
0 & 0 & 0 \\
1 & 0 & 1
\end{tabular}
\end{center}
This satisfies all of the field axioms.\\

\textbf{Def: }a \emph{vector space} over a field F is a set V with two operations $+:V \times V \rightarrow V$ and $\cdot : F\times V \rightarrow V$ (referred to as vector addition and scalar multiplication) satisfying the following properties

\begin{itemize}
\item $\forall\ v_1, v_2, v_3 \in V,\ (v_1 + v_2) + v_3 = v_1 + (v_2 + v_3)$
\item $\forall\ v_1, v_2 \in V,\ v_1 + v_2 = v_2 + v_1$
\item $\exists ^^21 \ 0 \in V,\ such\ that\ v + 0 = 0 + v\quad \forall\ v \in V$
\item $\forall\ v \in V,\ \exists\ w \in V,\ such\ that\ v + w = w + v = 0.\ We\ write\ v = -w$
\item $\forall\ v \in V,\ v \cdot 1 = 1 \cdot v = v$
\item $\forall\ v \in V,\ and\ \alpha,\beta \in F,\ \alpha \cdot (\beta \cdot v) = (\alpha \cdot \beta) \cdot v$
\item $\forall\ v \in V,\ and\ \alpha,\beta \in F,\ (\alpha + \beta) \cdot v = \alpha \cdot v + \beta \cdot v$
\item $\forall\ v_1, v_2 \in V,\ and\ \alpha \in F,\ \alpha \cdot (v_1 + v_2) = \alpha \cdot v_1 + \alpha \cdot v_2$
\item $\forall\ v_1, v_2 \in V,\ and\ \alpha \in F,\ \alpha \cdot v_1 + v_2 \in F$
\end{itemize}
These axioms looks suspiciously similar to the field axioms. In fact, \emph{any field is a vector space over itself}. By this construction, the vector space axioms are inherited by the field axioms. For example, the field $\mathbb{R}$ is a vector space over $\mathbb{R}$. To give an example of a vector space that's a bit more interesting, the set of 3 element vectors over the field $\mathbb{R}$ forms a vector space, denoted as $\mathbb{R}$$^3$. In general, the set of all n element vectors over the a field F form the vector space denoted as $F^n$.

\textbf{Def: }a \emph{subspace} of a vector space V is a subset $W \subseteq V$ such that W is also a vector space.\\\\
\textbf{Subspace Theorem: } \emph{Let V be a vector space over the field F, and let $W \subseteq V$ be nonempty. Then the following are equivalent.}
\begin{itemize}
\item W is a subspace of V
\item W is closed under addition and scalar multiplication
\item $\forall\ w_1, w_2 \in W\ and\ \alpha \in F,\ \alpha \cdot w_1 + w_2 \in W$
\end{itemize}
\emph{Proof: }1) $\Rightarrow$ 2) by vector space axioms. 2) $\Rightarrow$ 3)  by definition. Now assume 3). Take $\alpha = 1$ to get $w_1 + w_2 \in W\ \forall\ w_1, w_2 \in W$. Take $\alpha = -1$ to get $w_2 - w_1 \in W\ \forall\ w_1, w_2 \in W$. Finally, take $w_2 = 0_V \in W$ to get $\alpha \cdot w_1 \in W\ \forall\ \alpha \in F$ and $w_1 \in W$. Thus, W is closed under addition and scalar multiplication, so it is a subspace, and so 3) $\Rightarrow$ 1).\\

\textbf{Def: }let V be a vector space over a field F. A \emph{linear combination} of elements in V is a sum of multiples of elements of V. For example, $\alpha v_1 + \beta v_2 + \gamma v_3$ is a linear combination for $v_1,v_2,v_3 \in V$ and $\alpha, \beta, \gamma \in F$. A vector $v \in V$ is said to be a \emph{linear combination} if $\exists$ scalars $\alpha_i \in F$ and $\exists\ v_i \in V$ such that $v = \sum_{i}^{}\alpha_i v_i$.\\

\textbf{Def: }let V be a vector space over a field F, and $\beta =  \{ v_1 \dots v_n  \}$ a set of n vectors in V. The \emph{span} of $\beta$ is the set of all linear combinations of elements of $\beta$ with scalars from F.
$$span(v_1,\dots v_n) = \sum_{i = 1}^{n}\alpha_i v_i \quad \forall\ \alpha_i \in F,\ \forall\ v_i \in V$$
Note that if $v_1,\dots v_n \in V$, then $span(v_1,\dots v_n)$ is a subspace of V. It is a subset, and is closed under addition and scalar multiplication.\\

\textbf{Def: }let $W_1, W_2 \subseteq V$ be subspaces. We define the direct sum $$W_1 + W_2 = \{ w_1 + w_2\ :\ w_1 \in W_1,\ w_2 \in W_2 \} = span(W_1 \cup W_2)$$\\
More generally, if $W_i \subseteq V$ are subspaces indexed by I, we define$$\sum_{I}W_i = span\Bigg(\bigcup_{I}W_i \Bigg)$$\\
This is a subspace, due to the way in which we defined span. Note that if we had defined $+$ by taking the union of the two sets, this would not form a subspace, as the unions of two subspaces are not necessarily closed under addition between the two elements.

\textbf{Def: }a set of vectors in a vector space V is \emph{linearly independent} if $$\sum_{i = 1}^{n}\alpha_i v_i = 0 \Rightarrow \alpha_i = 0\ \forall \alpha_i \in F,\ \forall v_i \in V\ for\ not\ all\ v_i = 0$$
Likewise, a set of vectors is \emph{linearly dependent} if $$\exists\ \alpha_i \neq 0\ such\ that\ \sum_{i = 1}^{n}\alpha_i v_i = 0\ is\ true\ for\ not\ all\ v_i = 0$$\\
Linear (in)dependence is a big deal in linear algebra. Without diving into the mathematics of it (yet), linearly dependent vectors mean that some vector in the set can be expressed by other vectors in the set. In a sense, there is some repeated information. In contrast, each newly added linearly independent vector to a set adds new information.\\

\textbf{Toss In Theorem: }\emph{Let V be a vector space over F, and $\varnothing \neq S \subseteq V$ be a linearly independent subset of V. Suppose $v \in V$ is not in the span of S. Then $S \cup \{ v \}$ is linearly independent.}\\\\
\emph{Proof: }Suppose $S\ \cup\ \{ v \}$ is linearly dependent. Then $\exists\ v_1,\dots v_n \in S\ and\ \alpha_i,\dots \alpha_n \in F$ not all zero such that $$\alpha_1 v_1 + \dots + \alpha_n v_n + \alpha v= 0$$\\
Suppose $\alpha = 0$. Then our equation reads $\alpha_1 v_1 + \dots\ + \alpha_n v_n = 0$ for not all $\alpha_i = 0$. However, this contradicts the linear independence of S.\\

Now suppose $\alpha \neq 0$. Then $\alpha^{-1}$ exists, so we can multiply both sides and rearrange to arrive at $$v =  -\alpha^{-1} \alpha_1 v_1 - \dots\ - \alpha^{-1} \alpha_n v_n$$\\
The above is simply a linear combination of elements of S, so it is in the span of S. However, we initially assumed $v \notin span(S)$, and so we have arrived at a contradiction. Since all scenarios lead to a contradiction, we can conclude what we had assumed is false, proving that $S \cup \{ v \}$ is linearly independent.\\

\textbf{Def: }a \emph{basis} for a vector space V is a set of linearly independent vectors in V that spans V.\\\\
\textbf{Def: }for a given basis $\beta = \{ v_1, \dots v_n \}$ of a vector space V, the \emph{coordinates} refer to the scalars associated with the basis vectors. For example, an arbitrary $v \in V$ has representation $$v = \alpha_1 v_1 + \dots + \alpha_n v_n \quad \forall\ \alpha_i \in F$$\\as $\beta$ spans V. $\alpha_i$ is referred to as the "i'th" coordinate of $\beta$, associated with the basis vector $v_i$.

\end{document}





































